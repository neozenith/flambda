{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebc6b51-3243-4c8f-a376-f0cb2063866a",
   "metadata": {},
   "source": [
    "# NLP Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584ba113-1553-4958-85a5-b0d77275238a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "# Scientific\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# AWS\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sagemaker.huggingface\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk, Dataset, load_metric\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# Various Third Party\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bae4e8-5e1d-4150-85dd-c10bb209f708",
   "metadata": {},
   "source": [
    "## Setup Session Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10148cf3-3b61-4559-a58a-8ee0c66f802a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://play-projects-joshpeak/nlp-play/sagemaker\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "bucket = os.getenv(\"DEFAULT_BUCKET\")\n",
    "prefix = os.getenv(\"S3_PREFIX\")\n",
    "\n",
    "path_prefix = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "boto_session = boto3.Session(profile_name=os.getenv(\"AWS_PROFILE\"), region_name=os.getenv(\"AWS_DEFAULT_REGION\"))\n",
    "session = sagemaker.Session(boto_session=boto_session, default_bucket=bucket)\n",
    "s3fs = S3FileSystem(session=botocore.session.Session(profile=os.getenv(\"AWS_PROFILE\")))\n",
    "role = os.getenv(\"SAGEMAKER_ARN_ROLE\", None)\n",
    "\n",
    "print(path_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc157a3-2692-4539-9cef-97e632231f74",
   "metadata": {},
   "source": [
    "## Fetch Data from Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5003d5ae-7c57-4b1e-a312-38bd11b2c24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache miss... fetching...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " {'Groceries': 0,\n",
       "  'Fitness': 1,\n",
       "  'Pets/Vet': 2,\n",
       "  'Parking': 3,\n",
       "  'HomeLoan': 4,\n",
       "  'Health/Pharmacy': 5,\n",
       "  'Fuel': 6,\n",
       "  'Vehicle': 7,\n",
       "  'Home/Garden/Office': 8,\n",
       "  'Utility': 9,\n",
       "  'Cafe': 10,\n",
       "  'Newspaper': 11,\n",
       "  'Takeaway/Fastfood': 12},\n",
       " {0: 'Groceries',\n",
       "  1: 'Fitness',\n",
       "  2: 'Pets/Vet',\n",
       "  3: 'Parking',\n",
       "  4: 'HomeLoan',\n",
       "  5: 'Health/Pharmacy',\n",
       "  6: 'Fuel',\n",
       "  7: 'Vehicle',\n",
       "  8: 'Home/Garden/Office',\n",
       "  9: 'Utility',\n",
       "  10: 'Cafe',\n",
       "  11: 'Newspaper',\n",
       "  12: 'Takeaway/Fastfood'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_dataset = Path(\"./data/transactions.dataset/\")\n",
    "\n",
    "if not cache_dataset.exists():\n",
    "    print(\"Cache miss... fetching...\")\n",
    "    df = wr.athena.read_sql_query(\n",
    "        \"\"\"\n",
    "    SELECT \n",
    "        rule_based_label as label_string\n",
    "        , description as text\n",
    "    FROM finances.silver_labelled\n",
    "    WHERE rule_based_label not in ('Other')\n",
    "    \"\"\",\n",
    "        \"finances\",\n",
    "        boto3_session=boto_session,\n",
    "    )\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    ds.save_to_disk(str(cache_dataset))\n",
    "\n",
    "transactions_dataset = load_from_disk(str(cache_dataset))\n",
    "labels = list(transactions_dataset.to_pandas().label_string.unique())\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for i, l in enumerate(labels)}\n",
    "N = len(labels)\n",
    "N, label2id, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2205f-3e62-4a47-b15a-b14644024828",
   "metadata": {},
   "source": [
    "## Create Train / Test / Validate Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26350ec-87fa-4a18-8ba0-c34b58c8b80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label_string', 'text'],\n",
       "        num_rows: 909\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label_string', 'text'],\n",
       "        num_rows: 102\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_transactions_dataset = transactions_dataset.shuffle().train_test_split(test_size=0.1)\n",
    "train_test_transactions_dataset.cleanup_cache_files()\n",
    "train_test_transactions_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2cc23a-6eee-482a-9aa5-00512d157388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11af5412-3312-4047-9db3-e6b7bb824832",
   "metadata": {},
   "source": [
    "## Export Data to S3 ready for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9be4aac-a929-4362-83e5-6588624904d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://play-projects-joshpeak/nlp-play/sagemaker/transactions\n"
     ]
    }
   ],
   "source": [
    "path = f\"{path_prefix}/transactions\"\n",
    "print(path)\n",
    "# train_test_transactions_dataset.save_to_disk(path, fs=s3fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3176cd54-4416-4515-be56-e371bbd9b0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a93f5c-d817-489c-a96b-944d34e92c66",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e833f7-0893-48a7-9851-a993d9f0417c",
   "metadata": {},
   "source": [
    "## Load and Configure Base Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2e4ec9-90f3-4e62-9d14-138355177f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "config = AutoConfig.from_pretrained(model_name, label2id=label2id, id2label=id2label)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, ignore_mismatched_sizes=True, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9ee21-d1c9-4160-b042-0a4490f06de7",
   "metadata": {},
   "source": [
    "## Preprocess / Tokenize\n",
    "\n",
    "This uses HuggingFace's Dataset API to efficiently load batches into memory using Arrow and parallelizes the batches independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a0b74c5-a282-4dc3-ac12-f5e5477fca5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82597e65781143db955cdac8b62c290d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0b302a8c8d4359b2c018861c192e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label_string', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 909\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label_string', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 102\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessing_function(batch):\n",
    "    tokenized_batch = tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "    tokenized_batch[\"labels\"] = [label2id[label] for label in batch[\"label_string\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "\n",
    "tokenized_dataset = train_test_transactions_dataset.map(preprocessing_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139899c0-b7b0-4148-873f-3477ab30d47f",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "350d8b08-56b5-4bf0-97b5-2e4ecb814eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, label_string. If text, label_string are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/jpeak/play/flambda/.venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 909\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 285\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/285 01:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.255116</td>\n",
       "      <td>0.476978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690297</td>\n",
       "      <td>0.908591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.461206</td>\n",
       "      <td>0.931806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.353848</td>\n",
       "      <td>0.931806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.321781</td>\n",
       "      <td>0.931806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, label_string. If text, label_string are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 102\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, label_string. If text, label_string are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 102\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, label_string. If text, label_string are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 102\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, label_string. If text, label_string are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 102\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, label_string. If text, label_string are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 102\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=285, training_loss=0.8030652497944079, metrics={'train_runtime': 76.4867, 'train_samples_per_second': 59.422, 'train_steps_per_second': 3.726, 'total_flos': 21170476177020.0, 'train_loss': 0.8030652497944079, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cda3700-f685-46d8-b87a-d63894475893",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cca1a0de-9e83-42a5-bf3e-0db73495f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache miss... fetching...\n"
     ]
    }
   ],
   "source": [
    "others_cache_dataset = Path(\"./data/others_transactions.dataset/\")\n",
    "\n",
    "if not others_cache_dataset.exists():\n",
    "    print(\"Cache miss... fetching...\")\n",
    "    others_df = wr.athena.read_sql_query(\n",
    "        \"\"\"\n",
    "    SELECT \n",
    "        rule_based_label as label_string\n",
    "        , description as text\n",
    "    FROM finances.silver_labelled\n",
    "    WHERE rule_based_label in ('Other')\n",
    "    \"\"\",\n",
    "        \"finances\",\n",
    "        boto3_session=boto_session,\n",
    "    )\n",
    "    others_ds = Dataset.from_pandas(others_df)\n",
    "    others_ds.save_to_disk(str(others_cache_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9110160d-8f65-41f3-a3ab-89482560a6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d6c12d4fbf48cb9f6b600e0805d342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    }
   ],
   "source": [
    "def classify(batch):\n",
    "    tokenized_batch = tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "    # tokenized_batch[\"tokens\"] = tokenizer.tokenize(batch[\"text\"])\n",
    "    tokenized_batch[\"classification\"] = classifier(batch[\"text\"])\n",
    "    return tokenized_batch\n",
    "\n",
    "\n",
    "classified_others = others_ds.map(classify, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "929f6b56-814f-4807-8305-1a5b5b323b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_iter = classified_others.to_pandas().iterrows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88e47021-1c57-4938-8697-72d51e9becc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9,\n",
       "  label_string                                                  Other\n",
       "  text                                TULSI PETROLEUM P L KURRI KURRI\n",
       "  input_ids         [101, 10722, 4877, 2072, 11540, 1052, 1048, 13...\n",
       "  attention_mask    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...\n",
       "  classification    {'label': 'Groceries', 'score': 0.313800632953...\n",
       "  Name: 9, dtype: object),\n",
       " ['tu', '##ls', '##i', 'petroleum', 'p', 'l', 'ku', '##rri', 'ku', '##rri'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = next(other_iter)\n",
    "item, tokenizer.tokenize(item[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec20165-2a66-48fa-9a2d-de7e30638f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57b187-a7da-4a7c-a595-bc3b80fa4dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
