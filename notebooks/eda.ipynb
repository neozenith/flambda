{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebc6b51-3243-4c8f-a376-f0cb2063866a",
   "metadata": {},
   "source": [
    "# NLP Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584ba113-1553-4958-85a5-b0d77275238a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "# Scientific\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# AWS\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sagemaker.huggingface\n",
    "\n",
    "# HuggingFace\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk, Dataset, load_metric\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# Various Third Party\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bae4e8-5e1d-4150-85dd-c10bb209f708",
   "metadata": {},
   "source": [
    "## Setup Session Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10148cf3-3b61-4559-a58a-8ee0c66f802a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://play-projects-joshpeak/nlp-play/sagemaker\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "bucket = os.getenv(\"DEFAULT_BUCKET\")\n",
    "prefix = os.getenv(\"S3_PREFIX\")\n",
    "\n",
    "path_prefix = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "boto_session = boto3.Session(profile_name=os.getenv(\"AWS_PROFILE\"), region_name=os.getenv(\"AWS_DEFAULT_REGION\"))\n",
    "session = sagemaker.Session(boto_session=boto_session, default_bucket=bucket)\n",
    "s3fs = S3FileSystem(session=botocore.session.Session(profile=os.getenv(\"AWS_PROFILE\")))\n",
    "role = os.getenv(\"SAGEMAKER_ARN_ROLE\", None)\n",
    "\n",
    "print(path_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc157a3-2692-4539-9cef-97e632231f74",
   "metadata": {},
   "source": [
    "## Fetch Data from Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5003d5ae-7c57-4b1e-a312-38bd11b2c24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache miss... fetching...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " {'Groceries': 0,\n",
       "  'Utility': 1,\n",
       "  'Takeaway/Fastfood': 2,\n",
       "  'Parking': 3,\n",
       "  'Fuel': 4,\n",
       "  'Newspaper': 5,\n",
       "  'Health/Pharmacy': 6,\n",
       "  'Home/Garden/Office': 7,\n",
       "  'Cafe': 8,\n",
       "  'Fitness': 9,\n",
       "  'Pets/Vet': 10,\n",
       "  'Vehicle': 11,\n",
       "  'HomeLoan': 12},\n",
       " {0: 'Groceries',\n",
       "  1: 'Utility',\n",
       "  2: 'Takeaway/Fastfood',\n",
       "  3: 'Parking',\n",
       "  4: 'Fuel',\n",
       "  5: 'Newspaper',\n",
       "  6: 'Health/Pharmacy',\n",
       "  7: 'Home/Garden/Office',\n",
       "  8: 'Cafe',\n",
       "  9: 'Fitness',\n",
       "  10: 'Pets/Vet',\n",
       "  11: 'Vehicle',\n",
       "  12: 'HomeLoan'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_dataset = Path(\"./data/transactions.dataset/\")\n",
    "\n",
    "if not cache_dataset.exists():\n",
    "    print(\"Cache miss... fetching...\")\n",
    "    df = wr.athena.read_sql_query(\n",
    "        \"\"\"\n",
    "    SELECT \n",
    "        rule_based_label as label_string\n",
    "        , description as text\n",
    "    FROM finances.silver_labelled\n",
    "    WHERE rule_based_label not in ('Other')\n",
    "    \"\"\",\n",
    "        \"finances\",\n",
    "        boto3_session=boto_session,\n",
    "    )\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    ds.save_to_disk(str(cache_dataset))\n",
    "\n",
    "transactions_dataset = load_from_disk(str(cache_dataset))\n",
    "labels = list(transactions_dataset.to_pandas().label_string.unique())\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for i, l in enumerate(labels)}\n",
    "N = len(labels)\n",
    "N, label2id, id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2205f-3e62-4a47-b15a-b14644024828",
   "metadata": {},
   "source": [
    "## Create Train / Test / Validate Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26350ec-87fa-4a18-8ba0-c34b58c8b80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label_string', 'text'],\n",
       "        num_rows: 909\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label_string', 'text'],\n",
       "        num_rows: 102\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_transactions_dataset = transactions_dataset.shuffle().train_test_split(test_size=0.1)\n",
    "train_test_transactions_dataset.cleanup_cache_files()\n",
    "train_test_transactions_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2cc23a-6eee-482a-9aa5-00512d157388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11af5412-3312-4047-9db3-e6b7bb824832",
   "metadata": {},
   "source": [
    "## Export Data to S3 ready for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9be4aac-a929-4362-83e5-6588624904d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://play-projects-joshpeak/nlp-play/sagemaker/transactions\n"
     ]
    }
   ],
   "source": [
    "path = f\"{path_prefix}/transactions\"\n",
    "print(path)\n",
    "# train_test_transactions_dataset.save_to_disk(path, fs=s3fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3176cd54-4416-4515-be56-e371bbd9b0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a93f5c-d817-489c-a96b-944d34e92c66",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e833f7-0893-48a7-9851-a993d9f0417c",
   "metadata": {},
   "source": [
    "## Load and Configure Base Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2e4ec9-90f3-4e62-9d14-138355177f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([13, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([13]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "config = AutoConfig.from_pretrained(model_name, label2id=label2id, id2label=id2label)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, ignore_mismatched_sizes=True, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9ee21-d1c9-4160-b042-0a4490f06de7",
   "metadata": {},
   "source": [
    "## Preprocess / Tokenize\n",
    "\n",
    "This uses HuggingFace's Dataset API to efficiently load batches into memory using Arrow and parallelizes the batches independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a0b74c5-a282-4dc3-ac12-f5e5477fca5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd0213a89ec414793733b924872e5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d81551a313a4845977fcea67fa4e183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label_string', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 909\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label_string', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 102\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessing_function(batch):\n",
    "    tokenized_batch = tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "    tokenized_batch[\"labels\"] = [label2id[label] for label in batch[\"label_string\"]]\n",
    "    return tokenized_batch\n",
    "\n",
    "\n",
    "tokenized_dataset = train_test_transactions_dataset.map(preprocessing_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139899c0-b7b0-4148-873f-3477ab30d47f",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "350d8b08-56b5-4bf0-97b5-2e4ecb814eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: label_string, text. If label_string, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/jpeak/play/flambda/.venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 909\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 285\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/285 01:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.422814</td>\n",
       "      <td>0.488144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.814761</td>\n",
       "      <td>0.875576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.513699</td>\n",
       "      <td>0.921081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.384919</td>\n",
       "      <td>0.930652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.348806</td>\n",
       "      <td>0.930652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: label_string, text. If label_string, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 102\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: label_string, text. If label_string, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 102\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: label_string, text. If label_string, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 102\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: label_string, text. If label_string, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 102\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: label_string, text. If label_string, text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 102\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=285, training_loss=0.8363298382675438, metrics={'train_runtime': 79.9768, 'train_samples_per_second': 56.829, 'train_steps_per_second': 3.564, 'total_flos': 21170476177020.0, 'train_loss': 0.8363298382675438, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric(\"f1\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cda3700-f685-46d8-b87a-d63894475893",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cca1a0de-9e83-42a5-bf3e-0db73495f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache miss... fetching...\n"
     ]
    }
   ],
   "source": [
    "others_cache_dataset = Path(\"./data/others_transactions.dataset/\")\n",
    "\n",
    "if not others_cache_dataset.exists():\n",
    "    print(\"Cache miss... fetching...\")\n",
    "    others_df = wr.athena.read_sql_query(\n",
    "        \"\"\"\n",
    "    SELECT \n",
    "        rule_based_label as label_string\n",
    "        , description as text\n",
    "    FROM finances.silver_labelled\n",
    "    WHERE rule_based_label in ('Other')\n",
    "    \"\"\",\n",
    "        \"finances\",\n",
    "        boto3_session=boto_session,\n",
    "    )\n",
    "    others_ds = Dataset.from_pandas(others_df)\n",
    "    others_ds.save_to_disk(str(others_cache_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9110160d-8f65-41f3-a3ab-89482560a6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a344799d1ca4c8ead6c19db03fd381f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "category_widget = widgets.ToggleButtons(\n",
    "    options=labels,\n",
    "    description=\"Labels:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "def classify(batch):\n",
    "    tokenized_batch = tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "    # tokenized_batch[\"tokens\"] = tokenizer.tokenize(batch[\"text\"])\n",
    "    tokenized_batch[\"classification\"] = classifier(batch[\"text\"])\n",
    "    return tokenized_batch\n",
    "\n",
    "\n",
    "classified_others = others_ds.map(classify, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929f6b56-814f-4807-8305-1a5b5b323b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_others_df = classified_others.to_pandas()\n",
    "other_iter = classified_others_df.iterrows()\n",
    "item = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712a927-e63f-4dc6-8831-1034fee74ba3",
   "metadata": {},
   "source": [
    "## Data Labelling\n",
    "\n",
    "Use the below cell with Control+Enter.\n",
    "\n",
    " - Interactively loop through the Dataframe, using the model to try and predictively label the unlabelled entries.\n",
    " - Use the widget to select the correct label and run the cell\n",
    " - Running the cell grabs the value from the widget and applies the label before lodaing a new entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88e47021-1c57-4938-8697-72d51e9becc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Labelled THETUMN ROOMS COOKS HILL as Cafe...\n",
      "(1, label_string                                                   Cafe\n",
      "text                                       THETUMN ROOMS COOKS HILL\n",
      "input_ids         [101, 1996, 11667, 2078, 4734, 26929, 2940, 10...\n",
      "attention_mask    [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "classification    {'label': 'Groceries', 'score': 0.270081222057...\n",
      "Name: 1, dtype: object)\n",
      "==============================\n",
      "Please label this entry...\n",
      "\n",
      "(2, label_string                                                  Other\n",
      "text                              Belflora Newcastle Fl Broadmeadow\n",
      "input_ids         [101, 19337, 10258, 6525, 8142, 13109, 5041, 4...\n",
      "attention_mask    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...\n",
      "classification    {'label': 'Health/Pharmacy', 'score': 0.331919...\n",
      "Name: 2, dtype: object)\n",
      "['bel', '##fl', '##ora', 'newcastle', 'fl', 'broad', '##me', '##ado', '##w']\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0cdb38affa4d9f9c963874046bcc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Labels:', index=6, options=('Groceries', 'Utility', 'Takeaway/Fastfood', 'Parking',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if item is not None:\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Labelled {item[1].text} as {category_widget.value}...\")\n",
    "    classified_others_df.at[item[0], \"label_string\"] = category_widget.value\n",
    "    print(item)\n",
    "\n",
    "print(\"=\" * 30)\n",
    "print(\"Please label this entry...\\n\")\n",
    "item = next(other_iter)\n",
    "print(item)\n",
    "category_widget.value = item[1].classification[\"label\"]  # predictively try to label entry\n",
    "print(tokenizer.tokenize(item[1].text))\n",
    "print(\"=\" * 30)\n",
    "category_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ee5629f-5d57-44c3-8a30-8039543574ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_string</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home/Garden/Office</td>\n",
       "      <td>THE GAMES SHOP CHARLESTOWN</td>\n",
       "      <td>[101, 1996, 2399, 4497, 2798, 4665, 102, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>{'label': 'Groceries', 'score': 0.366621792316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cafe</td>\n",
       "      <td>THETUMN ROOMS COOKS HILL</td>\n",
       "      <td>[101, 1996, 11667, 2078, 4734, 26929, 2940, 10...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>{'label': 'Groceries', 'score': 0.270081222057...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label_string                        text  \\\n",
       "0  Home/Garden/Office  THE GAMES SHOP CHARLESTOWN   \n",
       "1                Cafe    THETUMN ROOMS COOKS HILL   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1996, 2399, 4497, 2798, 4665, 102, 0, 0,...   \n",
       "1  [101, 1996, 11667, 2078, 4734, 26929, 2940, 10...   \n",
       "\n",
       "                                      attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      classification  \n",
       "0  {'label': 'Groceries', 'score': 0.366621792316...  \n",
       "1  {'label': 'Groceries', 'score': 0.270081222057...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classified_others_df[classified_others_df.label_string != \"Other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e308fe-69cb-4d0d-ace8-31233f92ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save updated labelled dataset into silver_labelled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
